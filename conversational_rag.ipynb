{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a quick tutorial for a Naive Conversational RAG using Langchain"
      ],
      "metadata": {
        "id": "slPuiWrM0VMa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtOjdERITyuU"
      },
      "source": [
        "#### Installing necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikpfvnwuTyuU",
        "outputId": "f66f8a28-5268-4662-ce6f-6255522fcad7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.3.28 which is incompatible.\n",
            "langchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.147 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    langchain-openai \\\n",
        "    langchain-pinecone \\\n",
        "    openai \\\n",
        "    pinecone-client \\\n",
        "    PyMuPDF \\\n",
        "    tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GPT 3.5 Turbo as the LLM"
      ],
      "metadata": {
        "id": "QV4MRsIilp51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_LfrLvLJTyuU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai.chat_models.base import ChatOpenAI\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_key = \"YOUR_OPENAI_API_KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=openai_key,\n",
        "    model='gpt-3.5-turbo')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using LangChain SystemMessage to prime the behavior of this conversational RAG system"
      ],
      "metadata": {
        "id": "5x61ckIym-6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D6wkfLYSTyuV"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the PDF into Documents\n",
        "\n",
        "* Step 1: Extract text <br>\n",
        "Using pymupdf to extract text from PDF - In this exercise, I'm not using tables or figures.\n",
        "\n",
        "* Step 2: Pre-process text - Tokenize all the text, then create chunks <br>\n",
        "** Using tiktoken - an OpenSource tokenizer from OpenAI <br>\n",
        "** Using Encoding='cl100k_base' which is suitable for gpt-3.5-turbo <br>\n",
        "** For chunking: Using token-level chunking for simplicity. Small chunks may fragment sentences, while large chunks might include irrelevant context. For this exercise, chunk_size=512 (256-512 chunk size is [suggested](https://www.pinecone.io/learn/chunking-strategies/#Embedding-short-and-long-content) for text-embedding-ada-002).<br>\n",
        "\n",
        "* Step 3: Generate embeddings <br>\n",
        "Using the OpenAI model \"text-embedding-ada-002\". This generates 1536 dimensional vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "tOnRcHbDdVLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pinecone\n",
        "import pymupdf\n",
        "import tiktoken\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dRKRfS1rd7a3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract text\n",
        "\n",
        "class PDFLoader:\n",
        "    def __init__(self, pdf_path):\n",
        "        self.pdf_path = pdf_path\n",
        "\n",
        "    def extract_text(self):\n",
        "        doc = pymupdf.open(self.pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "\n",
        "loader = PDFLoader('attention.pdf')\n",
        "text = loader.extract_text()\n",
        "# print(text)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g6GSQBOoOz8X"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2, 3: Pre-process text, Generate embeddings\n",
        "\n",
        "class Preprocess:\n",
        "    def __init__(self):\n",
        "        openai_api_key = openai_key\n",
        "        if not openai_api_key:\n",
        "            raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n",
        "\n",
        "    def chunkify(self, text, chunk_size, encoding_name=\"cl100k_base\"):\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        tokens = encoding.encode(text)\n",
        "        return [encoding.decode(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "    def embed(self, chunks):\n",
        "        embeddings = []\n",
        "        for chunk in chunks:\n",
        "            response = openai.embeddings.create(\n",
        "                input=chunk,\n",
        "                model=embed_model.model\n",
        "            )\n",
        "            embeddings.append(response.data[0].embedding)\n",
        "        return embeddings\n",
        "\n",
        "    def chunkify_and_embed(self, text, chunk_size=512):\n",
        "        chunks = self.chunkify(text, chunk_size)\n",
        "        embeddings = self.embed(chunks)\n",
        "        return chunks, embeddings\n",
        "\n",
        "preprocessor = Preprocess()\n",
        "chunks, embeddings = preprocessor.chunkify_and_embed(text, chunk_size=512)\n"
      ],
      "metadata": {
        "id": "vnzQkoEWOMP4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (type(chunks), len(chunks))\n",
        "type(embeddings[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RwI_2P0PPZoh",
        "outputId": "7de7d3f9-27b3-4a4c-88af-741d50e60afb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WycvcmpGR7rh",
        "outputId": "3a543f70-b22f-4dbc-da41-6d3c42952fba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUQ066NFTyuW"
      },
      "source": [
        "### Creating Vector Store\n",
        "Using Pinecone free account.\n",
        "\n",
        "Step 1: Creating an index. Dimension = 1536 same as embeddings <br>\n",
        "Step 2: Upserting each chunk's embedding+metadata into the index. We are not making use of metadata for any filtering, but it's still good practice to maintain unique metadata in the vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Index and VectorStore creation\n",
        "\n",
        "import time\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "api_key = \"YOUR_PINECONE_API_KEY\"\n",
        "os.environ[\"PINECONE_API_KEY\"] = api_key\n",
        "pc = Pinecone(api_key=api_key)\n",
        "\n",
        "index_name = \"attention-vector-store\"\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "vector_store = PineconeVectorStore(index=index, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Adding Documents\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=text, metadata={\"source\": f\"document_{i}\"})\n",
        "    for i, text in enumerate(chunks)\n",
        "]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=[str(i) for i in range(len(documents))])\n",
        "index.describe_index_stats()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVjWH3Xyc88G",
        "outputId": "13d91448-5fae-43e0-9b97-642d70a890c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 20}},\n",
              " 'total_vector_count': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "Psp_ssTqWNcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top k Nearest Neighbor matches of the query from the vector store will be passed to the chatbot as context for the query. The query + context will be called the augmented prompt.\n",
        "\n",
        "### Conversational RAG\n",
        "For the chatbot to have conversational memory, we will append the augmented-prompt to the messages. This allows the bot to access conversational history.\n",
        "\n"
      ],
      "metadata": {
        "id": "GconMQy7WLAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QbLQUFl2Tyub"
      },
      "outputs": [],
      "source": [
        "def augment_prompt(query: str):\n",
        "    # Fetch top 3 matches\n",
        "    results = vector_store.similarity_search(query, k=3)\n",
        "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "    augmented_prompt = f\"\"\"Use the following pieces of context to answer the query at the end.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"What is a Transformer?\"\n",
        "    )\n",
        ")\n",
        "messages.append(prompt)\n",
        "res = chat(messages)\n",
        "print(res.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WBKGrl-VwBA",
        "outputId": "821b16ac-f26f-47ec-f734-354e0deef506"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Transformer is a model architecture that is based entirely on attention mechanisms, replacing the recurrent layers commonly used in encoder-decoder architectures. The Transformer model allows for more efficient parallelization and can establish a new state of the art in translation quality with significantly faster training times compared to models based on recurrent or convolutional layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> As we can see in the following code cell, the Chatbot is able to understand that 'it' in the query refers to a transformer: \"How does it compare with an RNN? List benefits or shortcomings.\" </b>"
      ],
      "metadata": {
        "id": "gXX7PVyOeAC0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lLIdGQ2YTyub",
        "outputId": "beeace17-9dfb-468f-b31c-b0d09a6068ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Transformer model differs from recurrent neural networks (RNNs) in several ways, offering both benefits and shortcomings when compared to RNN-based architectures:\n",
            "\n",
            "Benefits of Transformer over RNN:\n",
            "1. **Parallelization**: The Transformer model allows for significantly more parallelization compared to RNNs. This feature is crucial for training efficiency, especially with longer sequence lengths.\n",
            "2. **Training Speed**: Transformers can be trained faster than architectures based on RNNs due to their parallel nature, making them more time-efficient.\n",
            "3. **Global Dependencies**: Transformers utilize an attention mechanism to draw global dependencies between input and output, enabling them to model dependencies without regard to their distance in the sequences.\n",
            "4. **Interpretable Models**: Self-attention in Transformers could potentially yield more interpretable models, as individual attention heads in the model are observed to perform different tasks related to the syntactic and semantic structure of sentences.\n",
            "\n",
            "Shortcomings of Transformer compared to RNN:\n",
            "1. **Sequential Computation**: While the Transformer model offers benefits in terms of parallelization, the fundamental constraint of sequential computation in RNNs is overcome. However, there may be scenarios where sequential processing is advantageous.\n",
            "2. **Complexity**: Transformers may introduce additional complexity compared to RNNs, especially when considering the various components involved, such as attention mechanisms and multi-headed self-attention.\n",
            "3. **Training Data Size**: The effectiveness of Transformers may vary based on the size of the training data. Larger datasets may be required to fully leverage the benefits of the attention mechanisms in Transformers.\n",
            "4. **Model Interpretability**: While Transformers offer potential for interpretable models due to attention mechanisms, they may also introduce challenges in understanding and interpreting the behavior of these mechanisms compared to traditional RNNs.\n",
            "\n",
            "Overall, the Transformer model represents a significant departure from RNN-based architectures, offering advantages in terms of parallelization, training speed, and global dependency modeling, while also presenting challenges related to complexity and interpretability.\n"
          ]
        }
      ],
      "source": [
        "# Create a new augmented prompt and append to conversational history\n",
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"How does it compare with an RNN? List benefits or shortcomings.\"\n",
        "    )\n",
        ")\n",
        "messages.append(prompt)\n",
        "res = chat(messages)\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"How was its evaluation performed?\"\n",
        "    )\n",
        ")\n",
        "messages.append(prompt)\n",
        "res = chat(messages)\n",
        "print(res.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p9EaYZqeP6O",
        "outputId": "31a80129-2759-48a6-a2eb-bf5351880c63"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The evaluation of the Transformer model was performed on the English constituency parsing task. The Transformer was trained on the Wall Street Journal (WSJ) portion of the Penn Treebank, which consisted of about 40K training sentences. Additionally, it was trained in a semi-supervised setting using larger high-confidence and Berkeley Parser corpora with approximately 17M sentences. \n",
            "\n",
            "The evaluation involved training a 4-layer Transformer with a specific model size on the WSJ data and the semi-supervised data. Different vocabularies were used for these settings. The evaluation included selecting optimal dropout rates, attention mechanisms, residual connections, learning rates, and beam sizes on the Section 22 development set. Other parameters were kept consistent with the English-to-German base translation model during inference. \n",
            "\n",
            "The results of the evaluation were presented in a table, showing the performance of the Transformer model on English constituency parsing compared to other models in terms of WSJ 23 F1 score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FemN7HVETyub"
      },
      "source": [
        "Deleting index to save resources:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc.delete_index(index_name)\n"
      ],
      "metadata": {
        "id": "bxJZysYXYgQ0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some general Areas for Experimentation while developing a RAG Chatbot:\n",
        "1. Pre-processing -\n",
        "* * Choice of PDF Parsers\n",
        "** Choice of Tokenizer and encoding strategy\n",
        "** Chunking strategies - Chunking method (Semantic / sentence / token based), chunk overlap etc\n",
        "** Embedding models\n",
        "** Including Tables, Images\n",
        "\n",
        "2. Choice of LLM\n",
        "3. Deciding 'k' for Nearest Neighbor Search - Needs to be based on the token limit of LLM\n",
        "4. Choice of Vector databases\n",
        "5. Choice of different Retrieval methods: Self Query Retrieval (Metadata-based Filtering), Parent Document Retrieval etc\n",
        "6. Few-shot learning"
      ],
      "metadata": {
        "id": "XBAQRLIwjjTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources:\n",
        "https://towardsdatascience.com/advanced-retriever-techniques-to-improve-your-rags-1fac2b86dd61\n",
        "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb\n"
      ],
      "metadata": {
        "id": "dOKmwf6urLqk"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "redacre",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}